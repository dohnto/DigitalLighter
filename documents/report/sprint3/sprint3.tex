In this chapter the planning and work-flow regarding Sprint 3 will be described. 
Everything from the setting of our goals to implementation and testing. At the end team will evaluate the whole sprint, and try to answer on following questions: What went well? What could be improved?  
% TODO rewrite

\section{Sprint planning} \label{txt:sprint3_planning}
In the planning part of sprint 2, there have been introduced and idea or goal for refining and extending the existing code, customer agreed, with condition, that sprint 3 will be focused on image processing part.

As the image processing module was one of the major risks, there was done some preliminary research even in the end of sprint 2 about possible approaches.
One specific way to deal with the problem (using OpenCV and Hough transformation \cite{Duda:1972:UHT:361237.361242}) was introduced to customer in planning part of meeting.
Customer was not satisfied with such a low-level approach and wanted us to use some existing tools.
Therefore there was need for additional preliminary studies concerning existing projects or libraries that could be used.
You can read more about additional preliminary studies below.

Also there was made a proposal of additional "pre-demo" video showing the progress of implementation on Thursday 4th of October 2013 during the regular meeting and customer gladly accepted this offer.

All implementation related stories for sprint 3 are presented in Table \ref{tab:sprint3stories}.
\input{sprint3/stories.tex}

All the documentation related stories for sprint 3 are presented in Table \ref{tab:sprint3Documentationstories}.
\input{sprint3/storiesDocumentation.tex}
All the project management related stories for sprint 3 are presented in Table \ref{tab:sprint3storiesProcess}.
\input{sprint3/storiesProcess.tex}
% hous all in total: Estimated: 94+ 94 + 50= 238 Spent: 97+ 86+ 36 = 219

\subsection{Duration}
This sprint is 2 weeks long. From 30th of September 2013 to 13th of October 2013.
We agreed on the date of presentation and showing the running demo -- on Thursday 11 of October 2013.
Estimated velocity is 240 hours since we agreed on 30 working hours per person per week.

\section{Preliminary studies}
As explained in Section \ref{txt:sprint3_planning} the customer was presented the possible technical approaches which, if followed, might lead to the successful implementation of the image processing (device detection) module. The customer is well aware of the fact the image processing part itself should be considered rather complex and that it might be tricky to achieve good results while attempting to detect the lit devices' screens. Therefore he stressed out it was his strong requirement that the team would find, research and utilize already existing solutions.

One of the customers advices on this topic was to contact the project supervisor and ask him to either recommend the relevant department in NTNU or even better the specific persons who might be able to provide needed know-how or at least suggest which approach to choose in order to eventually obtain the most precise results. If the team were to fail getting the relevant information the reasonable amount of time should be spent on carrying out additional research regarding existing solutions, customer stated.

The team's supervisor suggested two persons working for IDI\footnote{\url{http://www.idi.ntnu.no/}} department at NTNU who might be of any help. Nevertheless, nor either of them or the other persons who they suggested were able to recommend any existing solution. Thus the team spend more time on researching the existing solutions using online accessible sources. The outcomes were as follows:

\subsection{Shape detection}
Since the objective is to detect the device's display which is lit and surrounded by the relatively dark surroundings (and thus sufficiently contrasting) the problem of detecting the device can be simplified to basically detecting the known shape - rectangle. Even though the detection of the specific shape is well described in many online sources including the OpenCV documentation\footnote{\url{http://docs.opencv.org/doc/tutorials/im
gproc/shapedescriptors/find_contours/find_contours.html}}. The algorithm is based on transforming the image into grayscale, blurring it, thresholding and detecting the edges. Though the team were not able to find any existing solution which could be used right away as the customer required.

\subsection{Color blob detection}
Another widely used approach comprises of detecting the areas of the specified color, so called \textit{color blobs}. This technique seemed to be even easier to implement as compared to the shape detection. What is more the libraries and working examples exist and those can be utilized conveniently.

\subsubsection{CVblob-for-android}
The open-source library CVblob-for-Andoid\footnote{\url{https://code.google.com/p/cvblob-for-android/}} was specifically designed for Android platform and it builds on the OpenCV library \textit{cvblob}\footnote{\url{https://code.google.com/p/cvblob/}}. It allows for the detection of the blobs of the specified color and it even features the center of mass estimation. Nevertheless after inspecting the code it was decided to choose the alternative code that provides the OpenCV among its functioning examples (see Section \ref{txt:sprint3_openCVcolorblob}) and which is easier to adjust.

\subsubsection{OpenCV color-blob-detection} \label{txt:sprint3_openCVcolorblob}
OpenCV4Android SDK includes a few examples demonstrating the capabilities of the library. One of them, \textit{color-blob-detection}\footnote{\url{https://github.com/Itseez/opencv/tree/master/samples/android/color-blob-detection}} turned out to suite the needs of DigitalLighter project the best. The example enables the user to select arbitrary color that would be used to detect color blobs. It was rather straightforward to adjust the source code so that the specifically chosen colors would be used to detect the devices' screens. Even though the obtained code cannot be considered as an already working technology as the customer required, given the outcomes of the research and the current knowledge of the team's members this was the most reasonable solution.




\section{Sprint goals}
The goal of this sprint is having a working application on a mobile phone that can work in two modes.  
Input type and output are for both modes common -- input is an image or video and output is location of detected mobile phones in a given matrix (e.g. 4x4 matrix) with color that mobile's screen is lighting.
In the first mode, real video from mobile's camera will be treated as an input and on the other hand in the second mode mock data (image or video) are treated as an input.

You can see example of input with matrix 2x2 in Figure \ref{img:sprint3_goal}. 
Appropriate output of device detection module is (left top tile is \texttt{[0,0]} and right top tile is \texttt{[1, 0]}): \texttt{\{blue,[0,1]\}, \{red,[0,0]\}, \{green,[1,1]\}}.

\begin{figure}[h]
	\centering
		\includegraphics[width=7cm,angle=90]{sprint3/sprint3_goal.pdf}
	\caption[Goal for sprint 3]{Example of ideal data input for device detection module with 2x2 matrix}
	\label{img:sprint3_goal}
\end{figure}

\section{Architecture} \label{txt:sprint3_architecture}
In this section it will be described device detection component module using 4+1 architectural view model.

\subsection{Logical view}
You can see a class diagram of new classes created in sprint 3 in Figure \ref{fig:class_diagram_sprint3}. 
We can divide these new classes into three categories. 

Into the first category we count classes \texttt{LightDetector}, \texttt{TileMapper} and \texttt{PointCollector}. 
These classes are a core of device detection module. 
The only visible method for working with this module is \texttt{PointCollector}'s method \texttt{collect}.
This function accepts two parameters: first is the image where mobile phone's screen detection should be done and second is a list of colors of screens which should be detected. 
Method \texttt{collect} is run asynchronously (as a new thread due to possible high time demanding operations) and therefore it was designed as a design pattern \texttt{Observer}
As class \texttt{PointCollector} implements interface \texttt{Observable} (also known as a \emph{Subject} \cite[p.~326]{Gamma:1995:DPE:186897}), its \texttt{Observers} must implement method \texttt{update}. 
To this function is passed as a argument hash map with keys as a colors and values as a lists of tile positions with appropriate color.
Class \texttt{LightDetector} is responsible for detecting location of color blobs in given image. 
Results as a list of pixel's position of blobs are passed as a return value of function \texttt{getBlobCoords}. 
These values are fetched by instance of \texttt{PointCollector} and passed to \texttt{TileMapper}, which is responsible for mapping points into appropriate tiles in grid.

In second category there is only class \texttt{CameraActivity}, which is responsible for handling outputs from camera or mock device and also gives appropriate feedback on screen of mobile (draws grid and marks detected blobs).

In last category there is class \texttt{ColorManager}, which is standalone class and in application there is only need for one instance. 
Therefore it was designed as a \texttt{Singleton} pattern \cite[p.~144]{Gamma:1995:DPE:186897}. 
Its main purpose is to handle transformations of different color formats such as special library, network and internal formats.

\begin{figure}[h]
	\centering
		\includegraphics[width=16.2cm]{sprint3/sprint3.png}
	\caption{Sprint 3 device detection module class diagram}
	\label{fig:class_diagram_sprint3}
\end{figure}

\subsection{Physical view}
You can see the physical view of whole product represented in Figure \ref{fig:sprint3_deployment_diagram}.
Even though in this particular case device \emph{Camera} part of \emph{Server} device, the architecture and code was designed generally so \emph{Camera} can be stand alone device.

\begin{figure}[h]
	\centering
		\includegraphics[width=15cm]{images/deployment-diagram-sprint3}
	\caption{Deployment diagram}
	\label{fig:sprint3_deployment_diagram}
\end{figure}

\subsection{Process view}
You can see the process view represented in Figures \ref{fig:sprint3_activity_diagram} and \ref{fig:sprint3_dfd}. The activity diagram, displayed in Figure \ref{fig:sprint3_activity_diagram}, can be perceived as a subactivity diagram of action \emph{detect clients location} from Figure \ref{fig:activity_diagram_server}.
It should be mentioned, that each time new image was taken by camera, new thread is created and therefore several processing in the same time can be performed.

\begin{figure}[h]
	\centering
		\includegraphics[width=16.2cm]{sprint3/activity_sprint3.pdf}
	\caption{Sprint 3 activity diagram}
	\label{fig:sprint3_activity_diagram}
\end{figure}


\begin{figure}[h]
	\centering
		\includegraphics[width=16.2cm]{sprint3/sprint3_dtd.pdf}
	\caption{Sprint 3 data flow diagram}
	\label{fig:sprint3_dfd}
\end{figure}

%\subsection{Development view}
%Since this is a single module, there is no need for development view

\section{Implementation}
\label{sec:sprint3_implementation}
Few problems during implementation had occurred. 
Since the OpenCV library for Java is in early stage, there is some functionality missing.
One of these is a method \texttt{open(String)} of class \texttt{VideoCapture}\footnote{\url{http://docs.opencv.org/java/org/opencv/highgui/VideoCapture.html}}.
In C++ version of OpenCV there exist such a method and it allows programmers to use a video for input for image processing.
After short research, a fix of this bug was found\footnote{\url{http://code.opencv.org/issues/3207}}, but this feature will be added in release 2.4.7.
Due to this discovery, there has been abandoned (at least until OpenCV 2.4.7 is released) a plan for mocking a video and only single pictures were used for testing.

\section{Testing}
A capability of Java to run on multiple platforms were utilized during testing and initial testing was performed without any Android device.
There have been created a testing set\footnote{\url{https://github.com/dohnto/DigitalLighter/tree/master/source/others/LightDetectorTester/res/drawable}} of simple images used for black box testing.

After this test, it has been decided to merge module with the rest of the application and perform some integration tests.
To outline a characteristics of further testing you can see videos \footnote{\url{http://www.youtube.com/watch?v=TcuMlvvAwSQ}} \footnote{\url{http://www.youtube.com/watch?v=fhWFAJY7QOg}} demonstrating current progress of implementation.

\section{Occurring risks}
This sprint, the \emph{dead end with technology} item from Table \ref{tab:risks} occurred. 
Since the required functionality (loading mocked videos) was proposed by customer mainly from testing reasons, the fact that the team met dead end was announced to customer with proposition, that the team can spend more time on researching other solutions or simply the final product will lack this feature.
The customer have chosen second choice.

\section{Customer feedback}
Since the implementation went better then expected the team was able to present the demonstration video already after one week. This video shows the requirements that Prototype 3 specifies (see Section \ref{txt:planning_productmilestones}) so the customer was pleased by the fact that the team is ahead of plan. It was therefore agreed that the team would attempt to fulfill at least some of the requirements that Prototype 4 specifies. After next week additional demonstration video was presented to the customer. It showed the ability of multiple clients to be recognized by the server simultaneously. Customer was satisfied with this solution.

During the implementation the team realized it might be tricky to achieve good results as far as the image processing is concerned. Thus it suggested that the acceptance tests could be presented to the customer in advanced so that he would be able to decide whether the project is being developed in the way he requires. Acceptance tests might be created in the form of the images or the conceptual videos that the system must be able to process and/or display correctly. The customer stated this is very mature suggestion and agreed with that.

The customer also added the user story regarding collecting the mobile devices for the final presentation as this might not be easy to achieve and it must be plan well ahead of time.

The first video presented to the customer can be found on YouTube under the name Prototype~2.1\footnote{\url{http://www.youtube.com/watch?v=TcuMlvvAwSQ}}. 
The second video is published under the name Prototype~3\footnote{\url{http://www.youtube.com/watch?v=fhWFAJY7QOg}}.

\section{Retrospective}
This section reflects on the past sprint. In order to learn from the mistakes done and thus to improve the workflow it is necessary to answer two essential questions: "What went well" and "What could be improved".
You can see the burn down chart in Figure \ref{fig:Burn3}.

\begin{figure}[h]
	\centering
		\includegraphics[width=14cm]{burndowns/sprint3.eps}
	\caption{Burn down chart for sprint 3}
	\label{fig:Burn3}
\end{figure}

\subsection{What went well}
Since the preliminary studies concerning image processing started very soon and instead of writing device detection module from scratch an existing code was used, the core of implementation was finished ahead of schedule. 
Also all documentation stories were reached according to plan.

\subsection{What could be improved}
Due to misunderstanding about customer's demands regarding using existing solution, extra time was required for additional preliminary studies resulting to the same output as the first preliminary study concerning image processing. 

In the end, the customer approved using OpenCV as a best option.
Therefore the communication with customer should be more precise and accurate and if any ambiguity occurs, it should be consulted with customer as soon as possible.

From the beginning it was obvious, that the device detection module's performance is affected by level of light.
During the testing the team was uncertain when the performance is sufficient, because the question of lighting was not discussed enough.
After this experience, an proposal to customer was made: if similar situation occurs, acceptance tests should be prepared and approved by the customer.

Last but not least, implementation story \textbf{I3.2} was not finished, due to \emph{dead end} described in Section \ref{sec:sprint3_implementation}.

Also, daily stand ups were often omitted due to late attendance.
Even though the workload is 30 hours per week per person, the efficiently spend time is much lower, therefore the workload was decreased to 25 hours per week per person (it better reflects the reality).
